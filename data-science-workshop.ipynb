{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f71ce02d-7f85-489f-b706-f30d9fd26f3d",
   "metadata": {},
   "source": [
    "**<center><font size=5>E.T. Signal Search üëΩ</font></center>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e6680e-dd7e-49c6-a66d-259f81368c03",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "----\n",
    "# Background\n",
    "\n",
    "##### **\"Are we alone in the Universe?\"**\n",
    "\n",
    "It's one of the most profound human questions.\n",
    "As technology improves, we're finding new and more powerful ways to seek for answers.\n",
    "\n",
    "The Breakthrough Listen team at Berkeley University, scans signals from millions of stars in the universe using world's most powerful telescopes.\n",
    "But it is not that easy - by the time, humans have built enormous numbers of radio device that can be detected as signals as well and distract our signals interpetation.\n",
    "\n",
    "In order to handle this challange, they are using two different methods:\n",
    "1. They intersperses scans of target stars that apprears in more than one regions of the sky - because it's probably isn't coming from the direction of target star.\n",
    "2. They discards signals that don't change their frequency - because it means that they are probably nearby the telescope.\n",
    "\n",
    "These two methods are quite effective, but they can be improved. The process today misses interesting signals. particularly those with complex time or frequency structure, and those in regions of the spectrum with lots of interference\n",
    "\n",
    "(from a [kaggle competition](https://www.kaggle.com/competitions/seti-breakthrough-listen)) <br></br>\n",
    "\n",
    "----\n",
    "\n",
    "**Table of Contents**\n",
    "- <a href='#1'>1. Project overview</a>\n",
    "  - <a href='#1_1'>1.1. Methodology</a>\n",
    "  - <a href='#1_2'>1.2. Imports, Methods, Paths, Reading definitions</a>\n",
    "- <a href='#2'>2. EDA: Exploratory Data Analysis</a>\n",
    "  - <a href='#2_1'>2.1. Load The Data</a>\n",
    "  - <a href='#2_2'>2.2. Visualize The Data</a>\n",
    "  - <a href='#2_3'>2.3. Summery Statistics</a>\n",
    "- <a href='#3'>3. Data Preprocessing</a>\n",
    "  - <a href='#3_1'>3.1. Data Augmentation</a>\n",
    "  - <a href='#3_2'>2.2. Feature Extraction</a>\n",
    "  - <a href='#3_3'>2.3. Data Splitting</a>\n",
    "\n",
    "  ----\n",
    "\n",
    "# <a id='1'>1. Project overview</a>\n",
    "\n",
    "We are about to use supervised learning skills to train and test a model that can help classify the presence of simulated extraterrestial signals in Breakthrough Listen team signal's scans.\n",
    "\n",
    "Because there are no confirmed examples of alien signals to use to train machine learning algorithms, the team included some simulated signals (that they call ‚Äú**needles**‚Äù) in the haystack of data from the telescope. They have identified some of the hidden needles so that we can train our model to find more.\n",
    "\n",
    "### <a id='1_1'>1.1. Methodology</a>\n",
    "\n",
    "1. EDA\n",
    "2. Instantiate GPU/TPU strategy\n",
    "3. Data preprocessing\n",
    "4. Build appropriate model\n",
    "5. Train and test it\n",
    "6. Conclusions\n",
    "\n",
    "### <a id='1_2'>1.2. Imports, Methods, Paths, Reading definitions</a>\n",
    "\n",
    "1. Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dc87ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install setigen\n",
    "#%pip install efficientnet\n",
    "#%pip install numpy\n",
    "#%pip install pandas\n",
    "#%pip install opencv-python\n",
    "#%pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e34c24",
   "metadata": {},
   "source": [
    "2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29082f57-7139-4513-8a08-6a635e562bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For operating system and file system\n",
    "import os\n",
    "\n",
    "# For data manipulation and analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# For Computer Vision Library\n",
    "import cv2 as cv\n",
    "\n",
    "# For Machine Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.applications import EfficientNetB2\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Conv2D\n",
    "import sklearn\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, GroupKFold, KFold\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# For SETI data manipulation\n",
    "#import setigen as stg\n",
    "from astropy import units as u\n",
    "\n",
    "# For data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import glob\n",
    "\n",
    "# For basic functionallity\n",
    "import math\n",
    "from enum import Enum\n",
    "from itertools import chain, permutations, combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f704bae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sometimes it can be necessary to re-run this command for plots to show automatically\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88388b96",
   "metadata": {},
   "source": [
    "3. Libraries Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaf1195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np version: 1.26.4\n",
      "pd version: 2.1.4\n",
      "tf version: 2.16.1\n",
      "sklearn version: 1.2.2\n"
     ]
    }
   ],
   "source": [
    "print('np version:',np.__version__)\n",
    "print('pd version:',pd.__version__)\n",
    "print('tf version:',tf.__version__)\n",
    "print('sklearn version:',sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac646b5",
   "metadata": {},
   "source": [
    "4. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075f1599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------#\n",
    "# Models\n",
    "# ------------------------------------#\n",
    "class Scheduler(Enum):\n",
    "    ReduceLROnPlateau = 1\n",
    "    CosineAnnealingLR = 2\n",
    "    CosineAnnealingWarmRestarts = 3\n",
    "\n",
    "class CFG:\n",
    "    # ------------------------------------#\n",
    "    # Basic\n",
    "    # ------------------------------------#\n",
    "    debug = False\n",
    "    scheduler = Scheduler.CosineAnnealingLR\n",
    "    epochs = 6\n",
    "    batch_size = 64\n",
    "    seed = 42\n",
    "    k_fold = 4\n",
    "\n",
    "    # ------------------------------------#\n",
    "    # Paths\n",
    "    # ------------------------------------#\n",
    "    base_folder_path = 'data'\n",
    "    train_folder_path = f'{base_folder_path}/train'\n",
    "    test_folder_path = f'{base_folder_path}/test'\n",
    "\n",
    "# ------------------------------------#\n",
    "# Datasets\n",
    "# ------------------------------------#\n",
    "training_set = pd.read_csv(f'{CFG.base_folder_path}/train_labels.csv')\n",
    "test_set = pd.read_csv(f'{CFG.base_folder_path}/sample_submission.csv')\n",
    "\n",
    "# ------------------------------------#\n",
    "# On Debug\n",
    "# ------------------------------------#\n",
    "if CFG.debug:\n",
    "    CFG.epochs = 1\n",
    "    CFG.batch_size = 10\n",
    "    training_set = training_set.sample(n = 100, random_state = CFG.seed).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520c526e",
   "metadata": {},
   "source": [
    "5. Utils Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6ba9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batched_data (data, batch_size, index):\n",
    "  return data[index * batch_size : (index + 1) * batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1751596",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# <a id='2'>2. EDA: Exploratory Data Analysis </a>\n",
    "\n",
    "\n",
    "With our libraries and configurations set up, we begin the exploratory data analysis or EDA.\n",
    "\n",
    "The **goal of EDA** is to understand the data underlying structure and patterns, identify important variables, detect outliers and anomalies and formulate hypotheses for further investigation. <br></br>\n",
    "\n",
    "This process will involve several steps including:\n",
    "1. Load the data\n",
    "2. Visualize the data\n",
    "3. Summery statistics\n",
    "\n",
    "\n",
    "### <a id='2_1'>2.1. Load The Data</a>\n",
    "\n",
    "Even though our task sounds like a *signal* detection problem, it is actually an\n",
    "*image* classification problem.\n",
    "\n",
    "The data we are provided with is radio spectrograms - a visual representation of the spectrum frequencies of a signal as it varies with time. <br></br>\n",
    "\n",
    "\n",
    "Our dataset actually contains:\n",
    "- a `train_labels.csv` file, which has our labels indicating the presence of an alien signal (referred to as needles) for each of the spectograms\n",
    "- `train` folder contains `.npy` files with the spectrograms stored as arrays\n",
    "- Similarly, `test` folder also contains `.npy` files with the test spectrograms stored as arrays\n",
    "- Of course, we are provided with a `test_labels.csv` file! <br></br>\n",
    "\n",
    "Let's check our `train` data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3d47e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000799a2b2c42d</td>\n",
       "      <td>0</td>\n",
       "      <td>data/train/0/0000799a2b2c42d.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00042890562ff68</td>\n",
       "      <td>0</td>\n",
       "      <td>data/train/0/00042890562ff68.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0005364cdcb8e5b</td>\n",
       "      <td>0</td>\n",
       "      <td>data/train/0/0005364cdcb8e5b.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0007a5a46901c56</td>\n",
       "      <td>0</td>\n",
       "      <td>data/train/0/0007a5a46901c56.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0009283e145448e</td>\n",
       "      <td>0</td>\n",
       "      <td>data/train/0/0009283e145448e.npy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id  target                         file_path\n",
       "0  0000799a2b2c42d       0  data/train/0/0000799a2b2c42d.npy\n",
       "1  00042890562ff68       0  data/train/0/00042890562ff68.npy\n",
       "2  0005364cdcb8e5b       0  data/train/0/0005364cdcb8e5b.npy\n",
       "3  0007a5a46901c56       0  data/train/0/0007a5a46901c56.npy\n",
       "4  0009283e145448e       0  data/train/0/0009283e145448e.npy"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_file_path(image_id, folder_path):\n",
    "  return '{}/{}/{}.npy'.format(folder_path, image_id[0], image_id)\n",
    "\n",
    "training_set['file_path'] = training_set['id'].apply(lambda x: get_file_path(image_id=x, folder_path=CFG.train_folder_path))\n",
    "test_set['file_path'] = test_set['id'].apply(lambda x: get_file_path(image_id=x, folder_path=CFG.test_folder_path))\n",
    "\n",
    "#FIXME: what is \"display\"? you can remove is\n",
    "display(training_set.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bd4c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_list = glob.glob(CFG.test_folder_path + \"/*/*.npy\")\n",
    "test_file_list.sort()\n",
    "test_set.assign(file=test_file_list).to_csv(\"test_label_file.csv\", index=False)\n",
    "test_df = pd.read_csv(\"test_label_file.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0940769",
   "metadata": {},
   "source": [
    "### <a id='2_2'>2.2. Visualize The Data</a>\n",
    "\n",
    "All the spectrograms are saved as numpy arrays. Typically, the data comes with extra metadata, but the organizes said that:\n",
    "\n",
    "> For the purposes of the Kaggle challenge, we have discarded the majority of the metadata and are simply presenting numpy arrays consisting of small regions of the spectrograms that we refer to as ‚Äúsnippets‚Äù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350bccf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spectrogram shape is: (6, 273, 256)\n"
     ]
    }
   ],
   "source": [
    "spectrogram_example = np.load(f'{CFG.base_folder_path}/example.npy')\n",
    "\n",
    "print(f'Spectrogram shape is: {spectrogram_example.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ed3f68",
   "metadata": {},
   "source": [
    "As we can see, the spectrogram has a shape of `(6, 273, 256)`. But, in order to undestand what does it mean, we need to dive deeper to how spectrogram works: <br></br>\n",
    "\n",
    "The main obstacle of searching for candidate signatures of extraterrestrial technology is our human technology - radio stations, wifi routers, cellphones and more also gives off radio signals.\n",
    "\n",
    "\n",
    "In order to isolate the relevant signals from human-generated ones, Breakthrough Listen team will look for **signals that appear to be coming from particular positions on the sky**. <br></br>\n",
    "\n",
    "\n",
    "Typically they do this by alternating observations of a primary target star with observations of three nearby stars:\n",
    "- 5 minutes on star ‚ÄúA‚Äù\n",
    "- then 5 minutes on star ‚ÄúB‚Äù\n",
    "- then back to star ‚ÄúA‚Äù for 5 minutes\n",
    "- then ‚ÄúC‚Äù\n",
    "- then back to ‚ÄúA‚Äù\n",
    "- then finishing with 5 minutes on star ‚ÄúD‚Äù.\n",
    "\n",
    "One set of six observations (ABACAD) is referred to as a *cadence*. <br></br>\n",
    "\n",
    "Let's look on a visual example for *cadence snippet*, from the interplanetary  [Voyager 1](https://en.wikipedia.org/wiki/Voyager_1) spacecraft:\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/kaggle-media/competitions/SETI-Berkeley/Screen%20Shot%202021-05-03%20at%2011.39.42.png\" width=\"600\">\n",
    "<br></br>\n",
    "\n",
    " The first, third, and fifth panels are the ‚ÄúA‚Äù target (the spacecraft, in this case). The yellow diagonal line is the radio signal coming from Voyager - it‚Äôs a diagonal line because the relative motion of the Earth and Doppler effect causing the frequency to change over time.\n",
    "\n",
    "\n",
    "\n",
    "More information about the spectrograms are provided [here](https://www.kaggle.com/c/seti-breakthrough-listen/overview/data-information). <br></br>\n",
    "\n",
    "Now we can plot a positive and negative *cadence snippets*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2214892d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrograms(cadence):\n",
    "    figure = plt.figure(figsize=(8, 5))\n",
    "    for i in range(6):\n",
    "        plt.subplot(6, 1, i + 1)\n",
    "        plt.imshow(cadence[i].astype(float), aspect='auto')\n",
    "    figure.text(0.5, 0.04, 'Frequency', ha='center', fontsize=8)\n",
    "    figure.text(0.04, 0.5, 'Time', va='center', rotation='vertical', fontsize=8)\n",
    "    plt.show()\n",
    "\n",
    "negative_example = spectrogram_example\n",
    "positive_example = np.load(f'{CFG.base_folder_path}/positive.npy')\n",
    "\n",
    "print('Here is a negative example:\\n')\n",
    "plot_spectrograms(negative_example)\n",
    "print('\\nHere is a positive one:\\n')\n",
    "plot_spectrograms(positive_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d516d963",
   "metadata": {},
   "source": [
    "2.3 Summery Statistics\n",
    "Add blockquote\n",
    "\n",
    "Let's create a histogram of the label frequencies in order to understand how rare is having a \"needle\" occurance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b373a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.hist()\n",
    "\n",
    "needles = len(training_set[training_set.target==1])\n",
    "\n",
    "print(f\"There are {needles} 'needles', which is {(needles / len(training_set) * 100)} % of the training set. \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e024a2f",
   "metadata": {},
   "source": [
    "Looks like our dataset is very imbalanced - This must be accounted for during modeling (for example, the loss function must be adjusted to account for the imbalance).\n",
    "\n",
    "----\n",
    "\n",
    "# <a id='3'>3. Data Preprocessing</a>\n",
    "\n",
    "There are a few things we can do to make the data more effiecient for our needs:\n",
    "\n",
    "1. **Data Cleaning** - we can remove noises from the data and keep only the relevant features of the data.\n",
    "2. **Data Augmentation** - we can apply data augmentation techniques to increase the diversity of the training dataset, like - rotation, flipping, scaling, or adding noise to the spectrograms.\n",
    "3. **Feature Extraction** - we can extract features that can help distinguishing between human-generated signals and potential alien signals, like - signal intensity, frequency characteristics and more\n",
    "4. **Data Splitting** - we need to split our train dataset into `training` and `validation` datasets in order to evaluate the performance of our model effectively and improve it, using K-fold cross-validation.\n",
    "\n",
    "\n",
    "### <a id='3_1'>3.1. Data Cleaning </a>\n",
    "\n",
    "\n",
    "##### <a id='3_1_1'>3.1.1 Fourier Transform </a>\n",
    "As we can see in the data visualizations - the horizontal lines are mostly noise and can be eliminated (the vertical or inclined lines are the ones that relevant for us)\n",
    "\n",
    "For filtering out horizontal lines, we can use `Fourier Transform`, but first - we need to understand its meaning\n",
    "\n",
    "\n",
    ">In the picture below, we can see the **Time Domain** representation of a audio signal, which shows the \"loudness\" (amplitude) of sounds wave changing with time. To better understand a audio signal, it is necessary to transform it into the **Frequency Domain** representation. This representation of a signal tells us what different frequencies are present in the signal.<br></br>\n",
    "  <img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*e-_z80BnbHWyFTfRLblJ_w.gif\" width=\"400\"> <br></br>\n",
    "> **Fourier Transform** is a mathematical concept that can *decompose a signal into its constituent frequencies*. In its 2D plot output, the x-axis represent the signal frequencies and the y-axis represent their magnitudes (amplitude size). There is also **Inverse Fourier Transform** concept, which is just the opposite of the Fourier Transform.<br></br>\n",
    " For a better understanding of Fourier Transform output - let's create two simple sine waves, with two different frequencies:  *amplitude = 1 and frequency = 3* and *amplitude = 2 and frequency = 11*.\n",
    " Now, we can combine them into a single signal, that will look like that - <br></br>\n",
    "  <img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*WBldOpArJgDXIFs5g_JydA.png\" width=\"400\"> <br></br>\n",
    "> The output of Fourier Transform will show two spikes  for the two frequencies and their magnitues <br></br>\n",
    "  <img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*aIyR6XoUYGJp0_3Ug6iEyA.png\" width=\"400\"> <br></br>\n",
    "for more information  [understand autio fft](https://towardsdatascience.com/understanding-audio-data-fourier-transform-fft-spectrogram-and-speech-recognition-a4072d228520)\n",
    "\n",
    "\n",
    "<br></br>\n",
    "Having said that, we can use *Fourier Transform* to mask the spectrogram's horizontal lines - they are represented as vertical lines in the central area of the fourier image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70272faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fourier_vertical_lines_mask(spectrogram):\n",
    "    spectrogram_frequencies_domain = np.fft.fft2(spectrogram)\n",
    "    # Shitf the zero-frequency to the center of the image, making it more intuitively\n",
    "    spectrogram_frequencies_domain = np.fft.fftshift(spectrogram_frequencies_domain)\n",
    "    # Mask the central area of the fourier image (probably a horizontal line)\n",
    "    spectrogram_frequencies_domain[:, 128:136] = 1\n",
    "\n",
    "    return spectrogram_frequencies_domain\n",
    "\n",
    "def plot_fourier_vertical_lines_mask(spectrogram, frequencies_domain):\n",
    "    _ , columns = plt.subplots(1,3,figsize=(8,8))\n",
    "\n",
    "    # Original Image\n",
    "    columns[0].imshow(spectrogram)\n",
    "    columns[0].set_title('Original Image', fontsize = 8);\n",
    "    # Fourier Transformation Image\n",
    "    columns[1].imshow(np.log(abs(frequencies_domain)))\n",
    "    columns[1].set_title('Fourier Image', fontsize = 8)\n",
    "    # Transformed Image using Inverse Fourier Transformation\n",
    "    columns[2].imshow(abs(np.fft.ifft2(frequencies_domain)))\n",
    "    columns[2].set_title('Transformed Image', fontsize = 8);\n",
    "\n",
    "def fourier_transform_preprocess(spectrograms):\n",
    "  masked_spectrograms = []\n",
    "  for spectrogram in spectrograms:\n",
    "    spectrogram = spectrogram.astype('float')/255\n",
    "    masked_spectrogram = fourier_vertical_lines_mask(spectrogram)\n",
    "    masked_spectrograms.append(abs(np.fft.ifft2(masked_spectrogram)))\n",
    "\n",
    "  return masked_spectrograms\n",
    "\n",
    "# Main\n",
    "spectrograms = [((np.load(f'{CFG.base_folder_path}/example.npy').astype('float'))/255)[0], \n",
    "                ((np.load(f'{CFG.base_folder_path}/data/positive.npy').astype('float'))/255)[0]\n",
    "]\n",
    "\n",
    "for spectrogram in spectrograms:\n",
    "  frequencies_domain = fourier_vertical_lines_mask(spectrogram)\n",
    "  plot_fourier_vertical_lines_mask(spectrogram, frequencies_domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179411e7",
   "metadata": {},
   "source": [
    "##### <a id='3_1_3'>3.1.3 Watershed Algorithm </a>\n",
    "\n",
    "In the space of traditional image segmentation methodologies, the Watershed Algorithm holds a significant place\n",
    "\n",
    "https://medium.com/@jaskaranbhatia/exploring-image-segmentation-techniques-watershed-algorithm-using-opencv-9f73d2bc7c5a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984aa3df",
   "metadata": {},
   "source": [
    "##### <a id='3_1_4'>3.1.4 Log </a>\n",
    "\n",
    "In this technic we are normalizing all the data to be between 0 and 1 therefore we are applying logarithm for all the values.\n",
    "\n",
    "There are also other ways to do it.\n",
    "\n",
    "We should pay attention that FFT is linear so for logarithmic image there may be bad results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1922537c",
   "metadata": {},
   "source": [
    "### <a id='3_2'>3.2. Data Augmentation </a>\n",
    "\n",
    "As we have discussed before the amount of positive data we have is very limited and therefore we decided we have to do some data augmentation.\n",
    "Data augmentation could be implemented in multiple ways,\n",
    "either by taking some example from our data and applying to it transformations for example rotation, translation, adding noise and so on or by generating new data.\n",
    "Generating new data is a bit more trickier as to so we have to be familiar with the data we are searching so the model will be trained on relevant data.\n",
    "\n",
    "After some digging we have encountered a library named `setigen` which can help us with the data augementation as it can generate by itself filterbank's that demoonstrate real one, and there are abilities of adding noise or signals of different forms.\n",
    "\n",
    "For more details,\n",
    "read:\n",
    "https://github.com/bbrzycki/setigen/blob/main/README.md  \n",
    "or  \n",
    "https://setigen.readthedocs.io/en/main/index.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6aa163",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from astropy import units as u\n",
    "from pathlib import Path\n",
    "import scipy\n",
    "\n",
    "import setigen as stg\n",
    "\n",
    "IMG_DIR = Path(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93fb7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sometimes it can be necessary to re-run this command for plots to show automatically\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df27124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_data = ((np.load('example.npy').astype('float'))/255)[0]\n",
    "my_data = abs(np.fft.ifft2(frequencies_domain))\n",
    "frame = stg.Frame.from_data(df=2.7939677238464355*u.Hz,\n",
    "                            dt=18.253611008*u.s,\n",
    "                            fch1=6095.214842353016*u.MHz,\n",
    "                            ascending=True,\n",
    "                            data=my_data)\n",
    "\n",
    "noise = frame.add_noise(x_mean=10, noise_type='chi2')\n",
    "\n",
    "signal = frame.add_signal(stg.sine_path(f_start=frame.get_frequency(200),\n",
    "                                        drift_rate=0.02*u.Hz/u.s,\n",
    "                                        period=100*u.s,\n",
    "                                        amplitude=5*u.Hz),\n",
    "                            stg.constant_t_profile(level=1),\n",
    "                            stg.box_f_profile(width=20*u.Hz),\n",
    "                            stg.constant_bp_profile(level=3))\n",
    "\n",
    "frame.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e83e84a",
   "metadata": {},
   "source": [
    "### <a id='3_3'>3.3. Data Splitting </a>\n",
    "\n",
    "In `K-fold cross-validation`, the original dataset is divided into K subsets (folds) of approximately equal size.\n",
    "Our model will train K times, each time using K-1 folds as the training data and the remaining fold as the validation data.\n",
    "\n",
    "This technique helps to provide a more reliable estimate of the model's performance compared to a single train-test split, as it uses multiple validation sets and reduces the variance in the performance estimate.\n",
    "\n",
    "Now, lets do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00367308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fold  target\n",
       "0     0         13500\n",
       "      1          1500\n",
       "1     0         13500\n",
       "      1          1500\n",
       "2     0         13500\n",
       "      1          1500\n",
       "3     0         13500\n",
       "      1          1500\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fold = StratifiedKFold(n_splits=k_fold, shuffle=True, random_state=seed)\n",
    "splitted_training_set = fold.split(train_dataset_labels, train_dataset_labels['target'])\n",
    "\n",
    "for fold_index, (train_index, validation_index) in enumerate(splitted_training_set):\n",
    "    train_dataset_labels.loc[validation_index, 'fold'] = int(fold_index)\n",
    "    train_dataset_labels['fold'] = train_dataset_labels['fold'].fillna(-1).astype(int)\n",
    "    \n",
    "display(train_dataset_labels.groupby(['fold', 'target']).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f0eb73",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# <a id='4'>4. TPU vs GPU</a>\n",
    "\n",
    "Before we are getting started, we need to identify whether our environment is equipped with a TPU or GPU for accelerating machine learning workloads.\n",
    "\n",
    "Here's a brief overview of TPU and GPU:\n",
    ">\n",
    "\n",
    "*   **TPU (Tensor Processing Unit)**:\n",
    "TPUs are custom-built accelerators developed by Google specifically for machine learning tasks, particularly those involving TensorFlow. They are highly optimized for matrix multiplication operations, which are prevalent in deep learning models.\n",
    "*   **GPU (Graphics Processing Unit)**:\n",
    "GPUs are specialized hardware originally designed for rendering graphics in video games but have been widely adopted for accelerating general-purpose computing tasks, including machine learning.\n",
    "\n",
    "> In summary, GPUs offer flexibility and high computational throughput for a wide range of tasks, while TPUs are specialized hardware accelerators optimized specifically for deep learning workloads, particularly those using TensorFlow.\n",
    "\n",
    "Having said that, let's detect what hardware do we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23df55a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A distributed strategy in TensorFlow is a mechanism for efficiently distributing the training of a machine\n",
    "# learning model across multiple devices.\n",
    "# For TPUs, we will use TPUStrategy - a strategy that specifically designed for training on TPUs.\n",
    "# For GPUs, we will use MirroredStrategy - the most common strategy for GPUs.\n",
    "\n",
    "try:\n",
    "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "  tf.config.experimental_connect_to_cluster(tpu)\n",
    "  tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "  strategy = tf.distribute.TPUStrategy(tpu)\n",
    "  print('Running on TPU')\n",
    "except ValueError:\n",
    "  strategy = tf.distribute.MirroredStrategy()\n",
    "  print('Running on GPU')\n",
    "  print('Num GPUs Available: ', len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "REPLICAS = strategy.num_replicas_in_sync\n",
    "print(f'REPLICAS: {REPLICAS}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91ae199",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# <a id='5'>5. Building A Model</a>\n",
    "\n",
    "We have decided to give multiple solutions for this competion.\n",
    "It will consist of 3 machine learning models:\n",
    "* Logistic Regression\n",
    "* SVM\n",
    "* Perceptron \n",
    "\n",
    "The common thing among all of those machine learning models mentioned above is that they are all supervised learning  \n",
    "algorithms which we are using them here for a binary classfication task, just for a reminder, our goal is to predict either 0 or 1  \n",
    "if there is some signals that seem to be signaled by some intelligence in some stars at the outer space.  \n",
    "  \n",
    "In additional we will will use a Deep learning model, based on `EfficientNetB2`.\n",
    "\n",
    "After submitting the results of all of the models we will compare between them.\n",
    "\n",
    "One of the main probelms we have encounter in this challenge is the amount of data it has for training, and although one of the solutions may be training the models on a smaller part of the data we wanted to do it in a different way.  \n",
    "For all 3 machine learning models, we will use the `sklearn.linear_model.SGDClassifier` class as it allows on-line learning with the `partial_fit` method.\n",
    "\n",
    "For the `EfficientNetB2` model we are also using batches @Coral, please go on \n",
    "\n",
    "### <a id='5_1'>5.1 A Simple Model</a>\n",
    "\n",
    "As mentioned above we will use 3 different machine learning models for the binary classification task.\n",
    "We will start with the `Logistic Regression`,\n",
    "In the `Logistic Regression` the relationship between the input features and the probability of the binary outcome is modeled using the logistic function the sigmoid function.\n",
    "\n",
    "<img src=\"https://helloacm.com/wp-content/uploads/2016/03/logistic-regression-example.jpg\" width=\"300\">\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf01e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think we can move it above and it could be used in the deep learning as well\n",
    "train_data = []\n",
    "for train_file_path in training_set['file_path']:\n",
    "    file = (np.load(train_file_path))\n",
    "    \n",
    "    # 1\n",
    "  #  on = np.vstack((file[0],file[2],file[4]))\n",
    "  #  off = np.vstack((file[1], file[3], file[5]))\n",
    "  #  file_merged = np.concatenate((on, off), axis=1).transpose((1, 0))\n",
    "\n",
    "\n",
    "    # 2\n",
    "    file_merged = np.vstack(file).transpose((1, 0))\n",
    "\n",
    "    # 3\n",
    "  #  np.vstack((file[0],file[1],file[2],file[3],file[4],file[5]))\n",
    "\n",
    "    # 4\n",
    "  #  on = np.vstack((file[0],file[2],file[4]))\n",
    "  #  off = np.vstack((file[1], file[3], file[5]))\n",
    "  #  file_merged = np.concatenate((on, off), axis=1))\n",
    "\n",
    "    train_data.append(file_merged.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7102012",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SGDClassifier(loss='log_loss')
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac040225",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=k_fold, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8466de81",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_scores = []\n",
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for train_index, test_index in kf.split(train_data, training_set['target']):\n",
    "\n",
    "    for i in train_index:       \n",
    "        X_train.append(train_data[i])\n",
    "        y_train.append(training_set['target'][i])\n",
    "\n",
    "    for i in test_index:       \n",
    "        X_test.append(train_data[i])\n",
    "        y_test.append(training_set['target'][i])\n",
    "\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        X_batch, y_batch = X_train[i:i+batch_size], y_train[i:i+batch_size]\n",
    "        model.partial_fit(X_batch, y_batch, classes=np.unique(training_set['target']))\n",
    "\n",
    "    # Evaluate the model\n",
    "    fold_score = model.score(X_test, y_test)\n",
    "    fold_scores.append(fold_score)\n",
    "\n",
    "print(\"Cross-validation scores for each fold:\", fold_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0e8a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think we can move it above and it could be used in the deep learning as well\n",
    "test_data = []\n",
    "for test_file_path in test_set['file_path']:\n",
    "    file = (np.load(test_file_path))\n",
    "    \n",
    "    # 1\n",
    "  #  on = np.vstack((file[0],file[2],file[4]))\n",
    "  #  off = np.vstack((file[1], file[3], file[5]))\n",
    "  #  file_merged = np.concatenate((on, off), axis=1).transpose((1, 0))\n",
    "\n",
    "\n",
    "    # 2\n",
    "    file_merged = np.vstack(file).transpose((1, 0))\n",
    "\n",
    "    # 3\n",
    "  #  np.vstack((file[0],file[1],file[2],file[3],file[4],file[5]))\n",
    "\n",
    "    # 4\n",
    "  #  on = np.vstack((file[0],file[2],file[4]))\n",
    "  #  off = np.vstack((file[1], file[3], file[5]))\n",
    "  #  file_merged = np.concatenate((on, off), axis=1))\n",
    "\n",
    "    test_data.append(file_merged.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341882f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_data = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15320b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.assign(target=predict_data)\n",
    "selected_column = [\"id\", \"target\"]\n",
    "final_result = test_df[selected_column]\n",
    "final_result.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f04ae8",
   "metadata": {},
   "source": [
    "### <a id='5_2'>5.2 A Deep Learning Model</a>\n",
    "We will use Keras to build our model.\n",
    "\n",
    "In Keras, `Sequence` is a class provided by the `keras.utils` module. It's commonly used for creating custom data generators for efficiently load and preprocess data in batches during training.\n",
    "\n",
    "For more information [tf.keras.utils.Sequnece](https://www.tensorflow.org/api_docs/python/tf/keras/utils/PyDataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8110dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data: the dataset that the data generator will iterate over\n",
    "# labels: the labels associated with the input data\n",
    "# batch_size: the number of samples to include in each batch\n",
    "# shuffle: A boolean value indicating whether to shuffle the data at the beginning of each epoch\n",
    "# preprocessing_function: Optional argument for specifing a function for data preprocessing\n",
    "\n",
    "class SETISequence(Sequence):\n",
    "    def __init__(self, data, labels, batch_size, shuffle):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    # Returns the total number of batches in the dataset\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.data) / self.batch_size)\n",
    "\n",
    "    # Returns a batch of data at a given index\n",
    "    def __getitem__(self, index):\n",
    "        data_ids_batch = get_batched_data(self.data, self.batch_size, index)\n",
    "        labels_batch = get_batched_data(self.labels, self.batch_size, index)\n",
    "        data_list = []\n",
    "\n",
    "        for id in data_ids_batch:\n",
    "          data = np.load(get_train_file_path(id))\n",
    "          data = data.astype('float')/255\n",
    "          data = fourier_transform_preprocess(data)\n",
    "          data_list.append(data)\n",
    "\n",
    "        data_batch = np.moveaxis(data_list, 1, -1)\n",
    "\n",
    "        return data_batch, labels_batch\n",
    "\n",
    "# Main\n",
    "#train = (pd.read_csv('data/train_labels.csv'))\n",
    "#train = train[train['id'].isin(['000cd479c2106d4', '000e869ed875e40', '00a1d9ce5045cd5', '00a1fbc9e9986d6', '00a3b4346d21596'])]\n",
    "#batch_size = 1 #32\n",
    "#dataset = SETISequence(train['id'], train['target'], batch_size, True)\n",
    "#dataset.__getitem__(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28cca5d",
   "metadata": {},
   "source": [
    "Now, we can initiate our Keras model. \n",
    "For this - we need to define our model architecture first.\n",
    "> *Model Architecture* - refers to the overall structure of our neural network.  It defines how the various \n",
    "layers are organized and connected to form a computational graph that transforms input data into output predictions.\n",
    "Model Architecture is builds from:\n",
    ">1. Input Layer - where the input data is fed into the network\n",
    ">2. Hidden Layers - the intermediate layers between the input and output layers where the actual computation takes place.\n",
    ">3. Output Layer - where the network produces its output predictions\n",
    ">4. Model Depth and Width - The depth of a neural network refers to the number of hidden layers it contains, \n",
    "while the width refers to the number of neurons in each layer.\n",
    "\n",
    "Now, we can take a look on our model layers:\n",
    "- We start with Convolutional layer - the building blocks of CNNs and are used to extract features from input images.\n",
    "The `Conv2D` layer using set of learnable filters (also known as kernels) to produce feature maps. In our code, the `Conv2D` layer has 3 filters with a \n",
    "kernel size of (3,3) and uses the ReLU activation function.\n",
    "- Then, we can use EfficientNetB2 architecture - a pretrained convolutional neural network (CNN) model, that initialized\n",
    "with weights pretrained on the ImageNet dataset.\n",
    "- The next layer is used to reduce the spatial dimensionality of the image extracted features before passing them to the fully connected layers\n",
    "- The next two layers would be Dense layers - which using ReLu activation function in order to capture complex patterns in the data.\n",
    "- And the last one - the output layer has a single neuron with sigmoid activation that produces a binary classification output (0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64c6579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb2_notop.h5\n",
      "\u001b[1m31790344/31790344\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 1us/step\n"
     ]
    }
   ],
   "source": [
    "# EfficientNetB2 must have 3 channels so we need to pre-proceess the data for it\n",
    "model = tf.keras.Sequential([\n",
    "        Conv2D(3,(3,3), strides=(1,1), padding='same', activation='relu', input_shape=(273,256,6)),\n",
    "        EfficientNetB2(input_shape=(273, 256, 6), weights='imagenet', include_top=False, drop_connect_rate=0.4),\n",
    "        GlobalAveragePooling2D(), \n",
    "        Dense(128, activation='relu'), \n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
